{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GANs_ch-17.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM1s1vfb5J6jdP5jsoJR9vr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RhysComissiong/Python-Machine-Learning-Tutorial/blob/master/GANs_ch_17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2hkmgE3_zMl",
        "outputId": "72449f76-c2d7-47da-c57c-cef1a80db2b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |██████████████████████████▋     | 350.2 MB 40.6 MB/s eta 0:00:02\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "\u001b[?25hTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 180, in _main\n",
            "    status = self.run(options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/req_command.py\", line 199, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 319, in run\n",
            "    reqs, check_supported_wheels=not options.target_dir\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 128, in resolve\n",
            "    requirements, max_rounds=try_to_avoid_resolution_too_deep\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 473, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 341, in resolve\n",
            "    name, crit = self._merge_into_criterion(r, parent=None)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 172, in _merge_into_criterion\n",
            "    if not criterion.candidates:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/structs.py\", line 139, in __bool__\n",
            "    return bool(self._sequence)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 143, in __bool__\n",
            "    return any(self)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 129, in <genexpr>\n",
            "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 33, in _iter_built\n",
            "    candidate = func()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 205, in _make_candidate_from_link\n",
            "    version=version,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 312, in __init__\n",
            "    version=version,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 151, in __init__\n",
            "    self.dist = self._prepare()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 234, in _prepare\n",
            "    dist = self._prepare_distribution()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 318, in _prepare_distribution\n",
            "    self._ireq, parallel_builds=True\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/prepare.py\", line 508, in prepare_linked_requirement\n",
            "    return self._prepare_linked_requirement(req, parallel_builds)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/prepare.py\", line 552, in _prepare_linked_requirement\n",
            "    self.download_dir, hashes\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/prepare.py\", line 243, in unpack_url\n",
            "    hashes=hashes,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/prepare.py\", line 102, in get_http_url\n",
            "    from_path, content_type = download(link, temp_dir.path)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/network/download.py\", line 158, in __call__\n",
            "    content_file.write(chunk)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/progress_bars.py\", line 107, in handle_sigint\n",
            "    self.original_handler(signum, frame)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main.py\", line 71, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 104, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 213, in _main\n",
            "    logger.debug(\"Exception information:\", exc_info=True)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1366, in debug\n",
            "    self._log(DEBUG, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1514, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1524, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1586, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 894, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.7/logging/handlers.py\", line 71, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1127, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1025, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 869, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/logging.py\", line 130, in format\n",
            "    formatted = super().format(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 616, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 566, in formatException\n",
            "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "  File \"/usr/lib/python3.7/traceback.py\", line 104, in print_exception\n",
            "    type(value), value, tb, limit=limit).format(chain=chain):\n",
            "  File \"/usr/lib/python3.7/traceback.py\", line 508, in __init__\n",
            "    capture_locals=capture_locals)\n",
            "  File \"/usr/lib/python3.7/traceback.py\", line 363, in extract\n",
            "    f.line\n",
            "  File \"/usr/lib/python3.7/traceback.py\", line 285, in line\n",
            "    self._line = linecache.getline(self.filename, self.lineno).strip()\n",
            "  File \"/usr/lib/python3.7/linecache.py\", line 16, in getline\n",
            "    lines = getlines(filename, module_globals)\n",
            "  File \"/usr/lib/python3.7/linecache.py\", line 47, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "  File \"/usr/lib/python3.7/linecache.py\", line 136, in updatecache\n",
            "    with tokenize.open(fullname) as fp:\n",
            "  File \"/usr/lib/python3.7/tokenize.py\", line 449, in open\n",
            "    encoding, lines = detect_encoding(buffer.readline)\n",
            "  File \"/usr/lib/python3.7/tokenize.py\", line 418, in detect_encoding\n",
            "    first = read_or_stop()\n",
            "  File \"/usr/lib/python3.7/tokenize.py\", line 376, in read_or_stop\n",
            "    return readline()\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "! pip install -q tensorflow-gpu==2.1.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1RUQmHoAyCK",
        "outputId": "ccc2b157-6b2a-4fce-cf74-5432efdf8eec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"GPU Available:\", tf.test.is_gpu_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsKXTDJuA5A7",
        "outputId": "c228f269-460f-492b-fa5f-5eb10f00c2a5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-3c7edf20f452>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "GPU Available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if tf.test.is_gpu_available():\n",
        "    device_name = tf.test.gpu_device_name()\n",
        "\n",
        "else:\n",
        "    device_name = '/CPU:0'"
      ],
      "metadata": {
        "id": "mZaD_1qVBApk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_LjgOukBbMU",
        "outputId": "836432be-2bc5-4498-8059-8eb8f9a16076"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "eZ0EOLHsBcy5",
        "outputId": "f0c41e84-f1a8-436f-d9c3-977bc8e4024c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c36f5579453e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m       \u001b[0muse_metadata_server\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_metadata_server\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m       ephemeral=ephemeral)\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server, ephemeral)\u001b[0m\n\u001b[1;32m    134\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 136\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    173\u001b[0m   request_id = send_request(\n\u001b[1;32m    174\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 175\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "-4BCdsw6BoJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## define a function for the generator:\n",
        "def make_generator_network(\n",
        "      num_hidden_layers=1,\n",
        "      num_hidden_units=100,\n",
        "      num_output_units=784):\n",
        "  \n",
        "  model = tf.keras.Sequential()\n",
        "  for i in range(num_hidden_layers):\n",
        "    model.add(\n",
        "        tf.keras.layers.Dense(units=num_hidden_units, use_bias=False))\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "    model.add(tf.keras.layers.Dense(units=num_output_units, activation='tanh'))\n",
        "  return model"
      ],
      "metadata": {
        "id": "lstvxL3_Euim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## define a function for the discriminator:\n",
        "def make_discriminator_network(\n",
        "      num_hidden_layers=1,\n",
        "      num_hidden_units=100,\n",
        "      num_output_units=1):\n",
        "  \n",
        "  model = tf.keras.Sequential()\n",
        "  for i in range(num_hidden_layers):\n",
        "    model.add(tf.keras.layers.Dense(units=num_hidden_units))\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "    model.add(tf.keras.layers.Dropout(rate=0.5))\n",
        "\n",
        "  model.add(\n",
        "      tf.keras.layers.Dense(\n",
        "          units=num_output_units, activation=None))\n",
        "  return model"
      ],
      "metadata": {
        "id": "YBdw1_jWFxb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = (28,28)\n",
        "z_size = 20\n",
        "mode_z = 'uniform' # 'uniform' vs 'normal'\n",
        "gen_hidden_layers=1\n",
        "gen_hidden_size=100\n",
        "disc_hidden_layers=1\n",
        "disc_hidden_size=100"
      ],
      "metadata": {
        "id": "lnkIv_LYQGxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1)"
      ],
      "metadata": {
        "id": "h5tj_P4_QnWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_model = make_generator_network(\n",
        "    num_hidden_layers=gen_hidden_layers,\n",
        "    num_hidden_units=gen_hidden_size,\n",
        "    num_output_units=np.prod(image_size))"
      ],
      "metadata": {
        "id": "EK-ZfeGPQpxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_model.build(input_shape=(None, z_size))\n",
        "gen_model.summary()"
      ],
      "metadata": {
        "id": "WhKeggLbRa7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disc_model = make_discriminator_network(\n",
        "    num_hidden_layers=disc_hidden_layers,\n",
        "    num_hidden_units=disc_hidden_size)"
      ],
      "metadata": {
        "id": "lxMKLKvORf_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disc_model.build(input_shape=(None, np.prod(image_size)))\n",
        "disc_model.summary()"
      ],
      "metadata": {
        "id": "01jJ-SexR3b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_bldr = tfds.builder('mnist')\n",
        "mnist_bldr.download_and_prepare()\n",
        "mnist = mnist_bldr.as_dataset(shuffle_files=False)"
      ],
      "metadata": {
        "id": "5B47bO_NSAua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(ex, mode='uniform'):\n",
        "  image = ex['image']\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "  image = tf.reshape(image, [-1])\n",
        "  image = image*2 - 1.0\n",
        "  if mode == 'uniform':\n",
        "    input_z = tf.random.uniform(\n",
        "        shape=(z_size,), minval=-1.0, maxval=1.0)\n",
        "  elif mode == 'normal':\n",
        "    input_z = tf.random.normal(shape=(z_size,))\n",
        "  return input_z, image"
      ],
      "metadata": {
        "id": "afT5tkMcGa_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_trainset = mnist['train']\n",
        "mnist_trainset = mnist_trainset.map(preprocess)"
      ],
      "metadata": {
        "id": "8LnRAIJ4HDjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_trainset = mnist_trainset.batch(32, drop_remainder=True)\n",
        "input_z, input_real = next(iter(mnist_trainset))\n",
        "print('input-z -- shape:    ', input_z.shape)\n",
        "print('input-real -- shape:', input_real.shape)"
      ],
      "metadata": {
        "id": "fARi7_MBHL61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g_output = gen_model(input_z)\n",
        "print('Output of G -- shape:', g_output.shape)"
      ],
      "metadata": {
        "id": "4cpvF_qwIdO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_logits_real = disc_model(input_real)\n",
        "d_logits_fake = disc_model(g_output)\n",
        "print('Disc. (real) -- shape:', d_logits_real.shape)\n",
        "print('Disc. (fake) -- shape:', d_logits_fake.shape)"
      ],
      "metadata": {
        "id": "ZLbE8fgdInyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the GAN model**"
      ],
      "metadata": {
        "id": "dvoAo87iJX03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "## Loss for the Generator\n",
        "g_labels_real = tf.ones_like(d_logits_fake)\n",
        "g_loss = loss_fn(y_true=g_labels_real, y_pred=d_logits_fake)\n",
        "print('Generator Loss: {:.4f}'.format(g_loss))"
      ],
      "metadata": {
        "id": "-rRCk7l_JaM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Loss for the Discriminator\n",
        "d_labels_real = tf.ones_like(d_logits_real)\n",
        "d_labels_fake = tf.zeros_like(d_logits_fake)\n",
        "\n",
        "d_loss_real = loss_fn(y_true=d_labels_real,\n",
        "                      y_pred=d_logits_real)\n",
        "\n",
        "d_loss_fake = loss_fn(y_true=d_labels_fake,\n",
        "                      y_pred=d_logits_fake)\n",
        "\n",
        "print('Discriminator Losses: Real {:.4f} Fake {:.4f}'.format(d_loss_real.numpy(), d_loss_fake.numpy()))"
      ],
      "metadata": {
        "id": "KAnGJcWx8RYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "num_epochs = 100\n",
        "batch_size = 64\n",
        "image_size = (28, 28)\n",
        "z_size = 20\n",
        "mode_z = 'uniform'\n",
        "gen_hidden_layers = 1\n",
        "gen_hidden_size = 100\n",
        "disc_hidden_layers = 1\n",
        "disc_hidden_size = 100\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "np.random.seed(1)\n",
        "\n",
        "if mode_z == 'uniform':\n",
        "  fixed_z = tf.random.uniform(\n",
        "      shape=(batch_size, z_size),\n",
        "      minval=-1, maxval=1)\n",
        "elif mode_z == 'normal':\n",
        "  fixed_z = tf.random.normal(\n",
        "    shape=(batch_size, z_size))\n",
        "  \n",
        "def create_samples(gmodel, input_z):\n",
        "  g_output = g_model(input_z, training=False)\n",
        "  images = tf.reshape(g_output, (batch_size, *image_size))\n",
        "  return (images+1)/2.0\n",
        "\n",
        "## Set-up the dataset\n",
        "mnist_trainset = mnist['train']\n",
        "mnist_trainset = mnist_trainset.map(\n",
        "    lambda ex: preprocess(ex, mode=mode_z)\n",
        ")\n",
        "\n",
        "mnist_trainset = mnist_trainset.shuffle(10000)\n",
        "mnist_trainset = mnist_trainset.batch(batch_size, drop_remainder=True)\n",
        "\n",
        "## Set-up the model\n",
        "with tf.device(device_name):\n",
        "  gen_model = make_generator_network(\n",
        "      num_hidden_layers=gen_hidden_layers,\n",
        "      num_hidden_units=gen_hidden_size,\n",
        "      num_output_units=np.prod(image_size))\n",
        "  gen_model.build(input_shape=(None, z_size))\n",
        "\n",
        "  disc_model = make_discriminator_network(\n",
        "      num_hidden_layers=disc_hidden_layers,\n",
        "      num_hidden_units=disc_hidden_size)\n",
        "  disc_model.build(input_shape=(None, np.prod(image_size)))\n",
        "\n",
        "## Loss function and optimizers:\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "g_optimizer = tf.keras.optimizers.Adam()\n",
        "d_optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "all_losses = []\n",
        "all_d_vals = []\n",
        "epoch_samples = []\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(1, num_epochs+1):\n",
        "\n",
        "  epoch_losses, epoch_d_vals = [], []\n",
        "\n",
        "  for i, (input_z, input_real) in enumerate(mnist_trainset):\n",
        "\n",
        "    ## Compute generator's loss\n",
        "    with tf.GradientTape() as g_tape:\n",
        "      g_output = gen_model(input_z)\n",
        "      d_logits_fake = disc_model(g_output,\n",
        "                                 training=True)\n",
        "      labels_real = tf.ones_like(d_logits_fake)\n",
        "      g_loss = loss_fn(y_true=labels_real,\n",
        "                       y_pred=d_logits_fake)\n",
        "      \n",
        "    ## Compute the gradients of g_loss\n",
        "    g_grads = g_tape.gradient(g_loss,\n",
        "                              gen_model.trainable_variables)\n",
        "    \n",
        "    ## Optimization: Apply the gradients\n",
        "    g_optimizer.apply_gradients(\n",
        "        grads_and_vars=zip(g_grads,gen_model.trainable_variables)\n",
        "    )\n",
        "\n",
        "    ## Compute discriminator's loss\n",
        "    with tf.GradientTape() as d_tape:\n",
        "      d_logits_real = disc_model(input_real, training=True)\n",
        "\n",
        "      d_labels_real = tf.ones_like(d_logits_real)\n",
        "\n",
        "      d_loss_real = loss_fn(y_true=d_labels_real, y_pred=d_logits_real)\n",
        "\n",
        "      d_logits_fake = disc_model(g_output, training=True)\n",
        "      d_labels_fake = tf.zeros_like(d_logits_fake)\n",
        "\n",
        "      d_loss_fake = loss_fn(y_true=d_labels_fake, y_pred=d_logits_fake)\n",
        "      d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "    ## Compute the gradients of d_loss\n",
        "    d_grads = d_tape.gradient(d_loss, disc_model.trainable_variables)\n",
        "\n",
        "    ## Optimization: Apply the gradients\n",
        "    d_optimizer.apply_gradients(\n",
        "        grads_and_vars=zip(d_grads, disc_model.trainable_variables)\n",
        "    )\n",
        "\n",
        "    epoch_losses.append(\n",
        "        (g_loss.numpy(), d_loss.numpy(),\n",
        "         d_loss_real.numpy(), d_loss_fake.numpy())\n",
        "    )\n",
        "\n",
        "    d_probs_real = tf.reduce_mean(tf.sigmoid(d_logits_real))\n",
        "    d_probs_fake = tf.reduce_mean(tf.sigmoid(d_logits_fake))\n",
        "    epoch_d_vals.append((d_probs_real.numpy(), d_probs_fake.numpy()))\n",
        "    all_losses.append(epoch_losses)\n",
        "    all_d_vals.append(epoch_d_vals)\n",
        "    print(\n",
        "        'Epoch {:03d} | ET {:.2f} min | Avg Losses >>'\n",
        "        ' G/D {:.4f}/{:.4f} [D-Real: {:.4f} D-Fake: {:.4f}]'\n",
        "        .format(\n",
        "            epoch, (time.time() - start_time)/60,\n",
        "            *list(np.mean(all_losses[-1], axis=0))))\n",
        "    epoch_samples.append(\n",
        "        create_samples(gen_model, fixed_z).numpy())"
      ],
      "metadata": {
        "id": "h-xq1u1687GO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "fig = plt.figure(figsize=(16, 6))\n",
        "\n",
        "## Plotting the losses\n",
        "ax = fig.add_subplot(1,2,1)\n",
        "g_losses = [item[0] for item in itertools.chain(*all_losses)]\n",
        "d_losses = [item[1]/2.0 for item in itertools.chain(*all_losses)]\n",
        "plt.plot(g_losses, label='Generator loss', alpha=0.95)\n",
        "plt.plot(d_losses, label='Discriminator loss', alpha=0.95)\n",
        "plt.legend(fontsize=20)\n",
        "\n",
        "ax.set_xlabel('Iteration', size=15)\n",
        "ax.set_ylabel('Loss', size=15)\n",
        "\n",
        "epochs = np.arange(1, 101)\n",
        "epoch2iter = lambda e: e*len(all_losses[-1])\n",
        "epoch_ticks = [1, 20, 40, 60, 80, 100]\n",
        "newpos = [epoch2iter(e) for e in epoch_ticks]\n",
        "ax2 = ax.twiny()\n",
        "ax2.set_xticks(newpos)\n",
        "ax2.set_xticklabels(epoch_ticks)\n",
        "ax2.xaxis.set_ticks_position('bottom')\n",
        "ax2.xaxis.set_label_position('bottom')\n",
        "ax2.spines['bottom'].set_position(('outward', 60))\n",
        "ax2.set_xlabel('Epoch', size=15)\n",
        "ax2.set_xlim(ax.get_xlim())\n",
        "ax.tick_params(axis='both', which='major', labelsize=15)\n",
        "ax2.tick_params(axis='both', which='major', labelsize=15)\n",
        "\n",
        "## Plotting the outputs of the discriminator\n",
        "ax = fig.add_subplot(1, 2, 2)\n",
        "d_vals_real = [item[0] for item in itertools.chain(*all_d_vals)]\n",
        "d_vals_fake = [item[1] for item in itertools.chain(*all_d_vals)]\n",
        "plt.plot(d_vals_real, alpha=0.75, label=r'Real: $D(\\mathbf{x})$')\n",
        "plt.plot(d_vals_fake, alpha=0.75, label=r'Fake: $D(G(\\mathbf{z}))$')\n",
        "plt.legend(fontsize=20)\n",
        "ax.set_xlabel('Iteration', size=15)\n",
        "ax.set_ylabel('Discriminator output', size=15)\n",
        "\n",
        "ax2 = ax.twiny()\n",
        "ax2.set_xticks(newpos)\n",
        "ax2.set_xticklabels(epoch_ticks)\n",
        "ax2.xaxis.set_ticks_position('bottom')\n",
        "ax2.xaxis.set_label_position('bottom')\n",
        "ax2.spines['bottom'].set_position(('outward', 60))\n",
        "ax2.set_xlabel('Epoch', size=15)\n",
        "ax2.set_xlim(ax.get_xlim())\n",
        "ax.tick_params(axis='both', which='major', labelsize=15)\n",
        "ax2.tick_params(axis='both', which='major', labelsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "smcy0vKcGQCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_epochs = [1, 2, 4, 10, 50, 100]\n",
        "fig = plt.figure(figsize=(10, 14))\n",
        "for i,e in enumerate(selected_epochs):\n",
        "  for j in range(5):\n",
        "    ax = fig.add_subplot(6, 5, i*5+j+1)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    if j == 0:\n",
        "      ax.text(\n",
        "          -0.06, 0.5, 'Epoch {}'.format(e),\n",
        "          rotation=90, size=18, color='red',\n",
        "          horizontalalignment='right',\n",
        "          verticalalignment='center',\n",
        "          transform=ax.transAxes\n",
        "      )\n",
        "\n",
        "    image = epoch_samples[e-1][j]\n",
        "    ax.imshow(image, cmap='gray_r')"
      ],
      "metadata": {
        "id": "e6fOqhNdJCsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.show()"
      ],
      "metadata": {
        "id": "CM8Uzq4WJ6Gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dcgan_generator(\n",
        "    z_size=20,\n",
        "    output_size=(28, 28, 1),\n",
        "    n_filters=128,\n",
        "    n_blocks=2):\n",
        "  size_factor = 2**n_blocks\n",
        "  hidden_size=(output_size[0]//size_factor,\n",
        "               output_size[1]//size_factor)\n",
        "  \n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Input(shape=(z_size,)),\n",
        "\n",
        "      tf.keras.layers.Dense(\n",
        "          units=n_filters*np.prod(hidden_size),\n",
        "          use_bias=False),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.LeakyReLU(),\n",
        "      tf.keras.layers.Reshape(\n",
        "          (hidden_size[0], hidden_size[1], n_filters)),\n",
        "\n",
        "      tf.keras.layers.Conv2DTranspose(\n",
        "          filters=n_filters, kernel_size=(5, 5),\n",
        "          strides=(1,1), padding='same', use_bias=False),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.LeakyReLU()\n",
        "  ])\n",
        "\n",
        "  nf = n_filters\n",
        "  for i in range(n_blocks):\n",
        "    nf = nf // 2\n",
        "    model.add(\n",
        "        tf.keras.layers.Conv2DTranspose(\n",
        "            filters=nf, kernel_size=(5,5),\n",
        "            strides=(2,2), padding='same',\n",
        "            use_bias=False))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  model.add(\n",
        "      tf.keras.layers.Conv2DTranspose(\n",
        "          filters=output_size[2], kernel_size=(5,5),\n",
        "          strides=(1,1), padding='same', use_bias=False,\n",
        "          activation='tanh'))\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "0_cUMqpqOEUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dcgan_discriminator(\n",
        "    input_size=(28, 28, 1),\n",
        "    n_filters=64,\n",
        "    n_blocks=2):\n",
        "  \n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Input(shape=input_size),\n",
        "      tf.keras.layers.Conv2D(filters=n_filters, kernel_size=5, strides=(1, 1), padding='same'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.LeakyReLU()\n",
        "  ])\n",
        "\n",
        "  nf = n_filters\n",
        "  for i in range(n_blocks):\n",
        "    nf = nf*.2\n",
        "    model.add(\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=nf, kernel_size=(5,5),\n",
        "            strides=(2,2), padding='same'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "    model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "  model.add(\n",
        "      tf.keras.layers.Conv2D(\n",
        "          filters=1, kernel_size=(7,7),\n",
        "          padding='valid'))\n",
        "  \n",
        "  model.add(tf.keras.layers.Reshape((1,)))\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "ZtVbC8EZQTE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_bldr = tfds.builder('mnist')\n",
        "mnist_bldr.download_and_prepare()\n",
        "mnist = mnist_bldr.as_dataset(shuffle_files=False)"
      ],
      "metadata": {
        "id": "RQuQ9-DmTY0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(ex, mode='uniform'):\n",
        "  image = ex['image']\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "\n",
        "  image = image*2 - 1.0\n",
        "  if mode == 'uniform':\n",
        "    input_z = tf.random.uniform(\n",
        "        shape=(z_size,), minval=-1.0, maxval=1.0\n",
        "    )\n",
        "  elif mode == 'normal':\n",
        "    input_z = tf.random.normal(shape=(z_size,))\n",
        "  return input_z, image"
      ],
      "metadata": {
        "id": "i_1oLjNLWZp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_model = make_dcan_generator()"
      ],
      "metadata": {
        "id": "Xi1HvhpsW9G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_model.summary()"
      ],
      "metadata": {
        "id": "4bR4RgHGW_7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disc_model = make_dcgan_discriminator()\n",
        "disc_model.summary()"
      ],
      "metadata": {
        "id": "2o5DvIOcXCpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "leTUXDygXPu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing WGAN-GP to train the DCGAN model**"
      ],
      "metadata": {
        "id": "aSzsKiaHXTyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 100\n",
        "batch_size = 128\n",
        "image_size = (28, 28)\n",
        "z_size = 20\n",
        "mode_x = 'uniform'\n",
        "lambda_gp = 10.0\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "np.random.set_seed(1)\n",
        "\n",
        "## Set-up the dataset\n",
        "mnist_trainset = mnist['train']\n",
        "mnist_trainset = mnist_trainset.map(preprocess)\n",
        "\n",
        "mnist_trainset = mnist_trainset.shuffle(10000)\n",
        "mnist_trainset = mnist_trainset.batch(batch_size, drop_remainder=True)\n",
        "\n",
        "## Set-up the model\n",
        "with tf.device(device_name):\n",
        "  gen_model = make_dcgan_generator()\n",
        "  gen_model.build(input_shape=(None, z_size))\n",
        "\n",
        "  disc_model = make_dcgan_discriminator()\n",
        "  disc_model.build(input_shape=(None, np.prod(image_size)))"
      ],
      "metadata": {
        "id": "9UzV50l8Sdda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "## Optimizers:\n",
        "g_optimizer = tf.keras.optimizers.Adam(0.0002)\n",
        "d_optimizer = tf.keras.optimizers.Adam(0.0002)\n",
        "\n",
        "if mode_z == 'uniform':\n",
        "  fixed_z = tf.random.uniform(\n",
        "      shape=(batch_size, z_size), minval=-1, maxval=1\n",
        "  )\n",
        "elif mode_z == 'normal':\n",
        "  fixed_z = tf.random.normal(shape=(batch_size, z_size))"
      ],
      "metadata": {
        "id": "FC5-NbX6ZrR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_samples(g_model, input_z):\n",
        "  g_output = g_model(input_z, training=False)\n",
        "  images = tf.reshape(g_output, (batch_size, *image_size))\n",
        "  return (images+1)/2.0"
      ],
      "metadata": {
        "id": "ONJBKK5jaNxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_losses = []\n",
        "epoch_samples = []\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "\n",
        "  epoch_losses = []\n",
        "\n",
        "  for i, (input_z, input_real) in enumerate(mnist_trainset):\n",
        "    \n",
        "    with tf.GradientTape() as d_tape, tf.GradientTape() as g_tape:\n",
        "\n",
        "      g_output = gen_model(input_z, training=True)\n",
        "\n",
        "      d_critics_real = disc_model(input_real, training=True)\n",
        "      d_critics_fake = disc_model(g_output, training=True)\n",
        "\n",
        "      ## Compute generator's loss:\n",
        "      g_loss = -tf.math.reduce_mean(d_critics_fake)\n",
        "\n",
        "      ## compute discriminator's losses:\n",
        "      d_loss_real = -tf.math.reduce_mean(d_critics_real)\n",
        "      d_loss_fake = tf.math.reduce_mean(d_critics_fake)\n",
        "      d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "      ## Gradient-penalty:\n",
        "      with tf.GradientTape() as gp_tape:\n",
        "        alpha = tf.random.uniform(\n",
        "            shape=[d_critics_real.shape[0], 1, 1, 1],\n",
        "            minval=0.0, maxval=1.0\n",
        "        )\n",
        "        interpolated = (alpha*input_real + (1-alpha)*g_output)\n",
        "        gp_tape.watch(interpolated)\n",
        "        d_critics_intp = disc_model(interpolated)\n",
        "\n",
        "      grads_intp = gp_tape.gradient(d_critics_intp, [interpolated,])[0]\n",
        "      grads_intp_l2 = tf.sqrt(tf.reduce_sum(tf.square(grads_intp), axis=[1,2,3]))\n",
        "      grad_penalty = tf.reduce_mean(tf.square(grads_intp_l2 - 1.0))\n",
        "\n",
        "      d_loss = d_loss + lambda_gp*grad_penalty\n",
        "\n",
        "    ## Optimization: Compute the gradients apply them\n",
        "    d_grads = d_tape.gradient(d_loss, disc_model.trainable_variables)\n",
        "    d_optimizer.apply_gradients(grads_and_vars=zip(d_grads, disc_model.trainable_variables))\n",
        "\n",
        "    g_grads = g_tape.gradient(g_loss, gen_model.trainable_variables)\n",
        "    g_optimizer.apply_gradients(grads_and_vars=zip(g_grads, gen_model.trainable_variables))\n",
        "\n",
        "    epoch_losses.append(\n",
        "        (g_loss.numpy(), d_loss.numpy(), \n",
        "         d_loss_real.numpy(), d_loss_fake.numpy()))\n",
        "    \n",
        "    all_losses.append(epoch_losses)\n",
        "    print(\n",
        "        'Epoch {:03d} | ET {:.2f} min | Avg Losses >>'\n",
        "        ' G/D {:6.2f}/{:6.2f} [D-Real: {:6.2f} D-Fake: {:6.2f}]'\n",
        "        .format(\n",
        "            epoch, (time.time() - start_time)/60,\n",
        "            *list(np.mean(all_losses[-1], axis=0))))\n",
        "    epoch_samples.append(\n",
        "        create_samples(gen_model, fixed_z).numpy())"
      ],
      "metadata": {
        "id": "gB_yg8qbadWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_epochs = [1, 2, 4, 10, 50, 100]\n",
        "fig = plt.figure(figsize=(10,14))\n",
        "for i,e in enumerate(selected_epochs):\n",
        "  for j in range(5):\n",
        "    ax = fig.add_subplot(6, 5, i*5+j+1)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    if j == 0:\n",
        "      ax.text(-0.06, 0.5, 'Epoch {}'.format(e),\n",
        "              rotation=90, size=18, color='red',\n",
        "              horizontalalignment='right',\n",
        "              verticalalignment='center',\n",
        "              transform=ax.transAxes)\n",
        "      \n",
        "      image = epoch_samples[e-1][j]\n",
        "      ax.imshow(image, cmap='gray_r')"
      ],
      "metadata": {
        "id": "BXRq762LgVjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.show()"
      ],
      "metadata": {
        "id": "FjqLO65phRjO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}